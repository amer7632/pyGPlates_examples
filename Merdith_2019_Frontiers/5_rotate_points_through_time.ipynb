{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm okay, so we're up to the main proessing step. Essentially everything to this point has just been pre-processing data to get it into a nice format. Here we actually take that data and use the plate model to rotate it through time in order to populate the ocean basins. We also have a subduction zone test, because we need to know when our 'points' are subducted and turned off. I took this from the workflow of Williams (2019) (see https://github.com/siwill22/agegrid-0.1, and https://zenodo.org/record/3271360 and Williams et al. (in review). In principal the functionality is also quite similar to work by Krister Karlson at Oslo (Karlson et al. 2019).\n",
    "\n",
    "I think eventually it will be better if I can move most of this to a function, but for now it's here in it's dirty mess (sorry).\n",
    "\n",
    "References\n",
    "\n",
    "Karlsen, K.S, Domeier, M., Gaina, C., & Conrad, C. (2019, September 1). TracerTectonics (Version 1.0). Zenodo. http://doi.org/10.5281/zenodo.3383154\n",
    "\n",
    "Merdith, A.S., Atkins, S.E., and Tetley, M.G. (2019). Tectonic controls on carbon and serpentinite storage in subducted upper oceanic lithosphere for the past 320 Ma. Frontiers: Earth Science.\n",
    "\n",
    "Williams, S.E., (2019, July 8). siwill22/agegrid-0.1 v1-alpha (Version v1-alpha). Zenodo. http://doi.org/10.5281/zenodo.3271360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:1066: UserWarning: Duplicate key in file \"/Users/Andrew/.matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/Users/Andrew/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:1066: UserWarning: Duplicate key in file \"/Users/Andrew/.matplotlib/matplotlibrc\", line #4\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygplates\n",
    "import pickle\n",
    "import time as tme\n",
    "import itertools\n",
    "import filter_and_reconstruct_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/Andrew/Documents/PhD/Scripts/Python_Scripts/pyGPlates_examples/Merdith_2019_Frontiers/Sample_Data/'\n",
    "savedir = '/Users/Andrew/Documents/PhD/Scripts/Scripts_Output/Serpentinisation/output/'\n",
    "\n",
    "rotfile = '%sMatthews_etal_GPC_2016_410-0Ma_GK07.rot' % datadir\n",
    "topologies = ['%sMatthews_etal_GPC_2016_MesozoicCenozoic_PlateTopologies.gpmlz' % datadir,\n",
    "              '%sMatthews_etal_GPC_2016_Paleozoic_PlateTopologies.gpmlz' % datadir]\n",
    "\n",
    "rotation_model = pygplates.RotationModel(rotfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import previously made file of crustal properties\n",
    "#separate load so faster if we need to recheck stuff\n",
    "#C_storage = pickle.load(open('%s/C_storage_2020-01-23_PMSR.p' % datadir, 'rb'))\n",
    "C_storage = pickle.load(open('%s/C_storage_2020-03-13_PMSR.p' % datadir, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "--- 4.50797796249 seconds to timestep ---\n",
      "164\n",
      "# of points 774\n",
      "whats been added 768\n",
      "1542\n",
      "--- 9.08390712738 seconds to timestep ---\n",
      "--- 9.08423495293 seconds overall ---\n"
     ]
    }
   ],
   "source": [
    "#apologise for this mess\n",
    "#POSR or PMSR?\n",
    "a\n",
    "POSR = False\n",
    "#PMSR = True\n",
    "#set some parameters up\n",
    "\n",
    "#resolution of grids\n",
    "resolution = 1 #in degrees\n",
    "#final timestep\n",
    "end_time = 0\n",
    "#timestep\n",
    "timestep = 1\n",
    "#make rasters at these times\n",
    "times = np.arange(400,end_time-timestep,-(timestep))\n",
    "\n",
    "#inititate loop to extract data\n",
    "start_time = tme.time()\n",
    "for time in times:\n",
    "    #define all our times (previous time, current time, next time)\n",
    "    print time\n",
    "    next_timestep = time - timestep\n",
    "    previous_timestep = time + timestep\n",
    "    \n",
    "    #resolve topologies for rotating points at this timestep, and at the previous timestep\n",
    "    curr_resolved_topologies = []\n",
    "    prev_resolved_topologies = []\n",
    "    pygplates.resolve_topologies(topologies, rotation_model, curr_resolved_topologies, time)\n",
    "    pygplates.resolve_topologies(topologies, rotation_model,prev_resolved_topologies, previous_timestep)\n",
    "    \n",
    "    #if not first timestep, then we need to rotate points from the previous timestep to the current timestep\n",
    "    if time != np.max(times):\n",
    "        #time = 100, next_timestep = 99\n",
    "        #next becomes current, current becomes previous\n",
    "        prev_lat, prev_lon = curr_lat, curr_lon\n",
    "        curr_lat, curr_lon, curr_zvals = next_lat, next_lon, next_zvals\n",
    "        #partition plates and points\n",
    "        prev_plate_partitioner, prev_plateIDs = curr_plate_partitioner, curr_plateIDs\n",
    "        curr_plate_partitioner = pygplates.PlatePartitioner(curr_resolved_topologies, rotation_model)\n",
    "        #get new set of plate IDs for current timestep\n",
    "        curr_plateIDs = filter_and_reconstruct_points.get_plate_ID_of_points(curr_plate_partitioner,\n",
    "                                                                             curr_lat,\n",
    "                                                                             curr_lon)\n",
    "\n",
    "        #returns none if it falls into a topology gap then filters to remove it\n",
    "        curr_lat, curr_lon, curr_zvals = filter_and_reconstruct_points.filter_points_Nones(curr_lat, \n",
    "                                                                                           curr_lon,\n",
    "                                                                                           curr_plateIDs,\n",
    "                                                                                           curr_zvals)\n",
    "        #lets get the resolved boundares of our points to test against\n",
    "        #there should be no \\'Nones\\' present, since we got rid of them just before\n",
    "        \n",
    "        resolved_topologies_containing_prev_points = filter_and_reconstruct_points.find_polygon_of_points(prev_lat,\n",
    "                                                                            prev_lon,\n",
    "                                                                            prev_resolved_topologies)\n",
    "        #filter current points\n",
    "        curr_lat = filter(None, curr_lat)\n",
    "        curr_lon = filter(None, curr_lon)\n",
    "        curr_zvals = filter(None, curr_zvals)\n",
    "        curr_plateIDs = filter(None, curr_plateIDs)\n",
    "\n",
    "        for ind, i in enumerate(prev_plateIDs):\n",
    "            #there are more plate IDs in current that previous, because we have all the new points attached\n",
    "            #but because they are appended, they occur at the end, so indices of prev_plateIDs should correspond\n",
    "            #to indices of curr_plateIDs until we run out of prev_plateIDs\n",
    "            prev_inputlatLon = pygplates.LatLonPoint(prev_lat[ind], prev_lon[ind])\n",
    "            prev_latLonPoint = pygplates.convert_lat_lon_point_to_point_on_sphere(prev_inputlatLon)\n",
    "            curr_inputlatLon = pygplates.LatLonPoint(curr_lat[ind], curr_lon[ind])\n",
    "            curr_latLonPoint = pygplates.convert_lat_lon_point_to_point_on_sphere(curr_inputlatLon)\n",
    "            prev_point_rotation = rotation_model.get_rotation(time, i, previous_timestep)\n",
    "            curr_point_rotation = rotation_model.get_rotation(time, curr_plateIDs[ind], previous_timestep)\n",
    "\n",
    "            #this is our subduction test, after Williams et al. (in review)\n",
    "            if curr_plateIDs[ind] != i:\n",
    "                collision = filter_and_reconstruct_points.detect_collision(timestep,\n",
    "                                             curr_latLonPoint,\n",
    "                                             prev_latLonPoint,\n",
    "                                             curr_point_rotation,\n",
    "                                             prev_point_rotation,\n",
    "                                             curr_plateIDs[ind],\n",
    "                                             resolved_topologies_containing_prev_points[ind],\n",
    "                                             i)\n",
    "                if collision == True:\n",
    "                    #if they've collided then these points can be None'd\n",
    "                    curr_lat[ind] = None\n",
    "                    curr_lon[ind] = None\n",
    "                    curr_zvals[ind] = None\n",
    "                    curr_plateIDs[ind] = None\n",
    "                    \n",
    "        #originally some points were slipping through, so double sure #NoNones\n",
    "        curr_lat = filter(None, curr_lat)\n",
    "        curr_lon = filter(None, curr_lon)\n",
    "        curr_zvals = filter(None, curr_zvals)\n",
    "        curr_plateIDs = filter(None, curr_plateIDs)\n",
    "        \n",
    "        print '# of points', len(curr_lat)\n",
    "        \n",
    "        #now we can export our data\n",
    "        if time <= 200:\n",
    "            filter_and_reconstruct_points.exporting(curr_lon,curr_lat,curr_zvals, time, savedir, POSR)\n",
    "           #exporting values to xyz table\n",
    "        #now to rotate through time to next timestep\n",
    "        #re-initiate the next time step lat/lon/zvals, as we want to overwrite all previous 'nexts'\n",
    "        #(which are 'current')\n",
    "        #with the actual next timestep\n",
    "        next_lat = []\n",
    "        next_lon = []\n",
    "        next_zvals = []\n",
    "        for ind, i in enumerate(curr_plateIDs):\n",
    "\n",
    "                #print ind\n",
    "                #we have already accounted for points that have been subducted, so lets rotate them through time to generate next\n",
    "                #NB we output points for next timestep\n",
    "                curr_inputlatLon = pygplates.LatLonPoint(curr_lat[ind], curr_lon[ind])\n",
    "                curr_latLonPoint = pygplates.convert_lat_lon_point_to_point_on_sphere(curr_inputlatLon)\n",
    "                next_point_rotation = rotation_model.get_rotation(next_timestep, i, time)\n",
    "                next_reconstructed_point = next_point_rotation * curr_latLonPoint\n",
    "                tmp_lat = next_reconstructed_point.to_lat_lon_list()[0][0]\n",
    "                tmp_lon = next_reconstructed_point.to_lat_lon_list()[0][1]\n",
    "                #print tmp_lat, tmp_lon, next_reconstructed_point\n",
    "                next_lat.append(tmp_lat)\n",
    "                next_lon.append(tmp_lon)\n",
    "                next_zvals.append(curr_zvals[ind])\n",
    "    \n",
    "    #get new data at timestep (OR) initiate first data if at max(times)\n",
    "    MOR_lat = []\n",
    "    MOR_lon = []\n",
    "    MOR_zvals = []\n",
    "    MOR_plateIDs = []\n",
    "    init_curr_lat = []\n",
    "    init_curr_lon = []\n",
    "    init_curr_plateIDs = []\n",
    "    for i,j,k,l,m,n,o,p,q,r,s,t,u in itertools.izip(C_storage[time]['Start-Lat-Point'],\n",
    "                                                   C_storage[time]['End-Lat-Point'],\n",
    "                                                   C_storage[time]['Start-Lon-Point'],\n",
    "                                                   C_storage[time]['End-Lon-Point'],\n",
    "                                                   C_storage[time]['RightPlate'],\n",
    "                                                   C_storage[time]['LeftPlate'],\n",
    "                                                   C_storage[time]['C-prod'],\n",
    "                                                   C_storage[time]['C-prod_vertical'],\n",
    "                                                   C_storage[time]['Serpentinites_raw'],\n",
    "                                                   C_storage[time]['Serpentinites_raw_vertical'],\n",
    "                                                   C_storage[time]['Thickness Mean'],\n",
    "                                                   C_storage[time]['Vertical Area Mean'],\n",
    "                                                   C_storage[time]['Spreading Rate']):\n",
    "\n",
    "\n",
    "        a = s[0] #total thick\n",
    "        b = s[1] #upper thick\n",
    "        c = s[2] #lower thick\n",
    "        d = s[3] #trans thick\n",
    "        e = s[4] #dyke thick\n",
    "        f = s[5] #gabbro thick\n",
    "        \n",
    "        #this is if we're using the POSR.\n",
    "        if POSR:\n",
    "            if time > 160:\n",
    "                print'here'\n",
    "                #print 'using POSR'\n",
    "                #use vertical values if old, gets undone in subduction flux, but not perfectly\n",
    "                #this lets us detect them later on more easily\n",
    "                #divide by two so we don't double dip\n",
    "    \n",
    "                o = p * 1000000/2    #C\n",
    "                q = r * 1000000/2    #serp\n",
    "                a = t[0] * 1000000/2 #total thick\n",
    "                b = t[1] * 1000000/2 #upper thick\n",
    "                c = t[2] * 1000000/2 #lower thick\n",
    "                d = t[3] * 1000000/2 #trans thick\n",
    "                e = t[4] * 1000000/2 #dyke thick\n",
    "                f = t[5] * 1000000/2 #gabbro thick\n",
    "            \n",
    "        points = []\n",
    "        points.append((i,k))\n",
    "        points.append((j,l))\n",
    "        polyline = pygplates.PolylineOnSphere(points)\n",
    "        \n",
    "        #get stage rotations of plate IDs i.e. m and n\n",
    "        line_rotation_right = rotation_model.get_rotation(next_timestep, int(m), time)\n",
    "        line_rotation_left = rotation_model.get_rotation(next_timestep, int(n), time)\n",
    "        reconstructed_polyline_right = line_rotation_right * polyline\n",
    "        reconstructed_polyline_left = line_rotation_left * polyline\n",
    "        tessellated_polyline_right = reconstructed_polyline_right.to_tessellated(np.radians(2))\n",
    "        tessellated_polyline_left = reconstructed_polyline_left.to_tessellated(np.radians(2))\n",
    "        latlon_list_right = tessellated_polyline_right.to_lat_lon_list()\n",
    "        latlon_list_left = tessellated_polyline_left.to_lat_lon_list()\n",
    "        recon_lats_right, recon_lons_right = [i[0] for i in latlon_list_right], [i[1] for i in latlon_list_right]\n",
    "        recon_lats_left, recon_lons_left = [i[0] for i in latlon_list_left], [i[1] for i in latlon_list_left]\n",
    "        \n",
    "        #if time is max we have a separate class for 'initial values', this is just to initiate the sequence and\n",
    "        #is unused after the first loop, as we keep appending new values onto 'curr_lat etc.'\n",
    "        if time == np.max(times):\n",
    "            #print h\n",
    "            init_curr_tessellated_polyline_right = polyline.to_tessellated(np.radians(2))\n",
    "            init_curr_tessellated_polyline_left = polyline.to_tessellated(np.radians(2))\n",
    "            init_curr_latlon_list_right = init_curr_tessellated_polyline_right.to_lat_lon_list()\n",
    "            init_curr_latlon_list_left = init_curr_tessellated_polyline_left.to_lat_lon_list()\n",
    "            init_curr_recon_lats_right, init_curr_recon_lons_right = [i[0] for i in init_curr_latlon_list_right], [i[1] for i in init_curr_latlon_list_right]\n",
    "            init_curr_recon_lats_left, init_curr_recon_lons_left = [i[0] for i in init_curr_latlon_list_left], [i[1] for i in init_curr_latlon_list_left]\n",
    "\n",
    "            for i in range(len(init_curr_recon_lats_right)):\n",
    "                init_curr_lat.append(init_curr_recon_lats_right[i])\n",
    "                init_curr_lat.append(init_curr_recon_lats_left[i])\n",
    "                init_curr_lon.append(init_curr_recon_lons_right[i])\n",
    "                init_curr_lon.append(init_curr_recon_lons_left[i])\n",
    "                #init_curr_plateIDs.append(m)\n",
    "                #init_curr_plateIDs.append(n)\n",
    "                #update arrays with data at new timestep\n",
    "                #right then left\n",
    "                \n",
    "        for i in range(len(recon_lats_right)):\n",
    "            MOR_lat.append(recon_lats_right[i])\n",
    "            MOR_lat.append(recon_lats_left[i])\n",
    "            MOR_lon.append(recon_lons_right[i])\n",
    "            MOR_lon.append(recon_lons_left[i])\n",
    "            #have to do zvals twice, once for right ridge, second time for left ridge\n",
    "            #important, as otherwise only half of your ocean is populated\n",
    "            MOR_zvals.append([o,q,a,b,c,d,e,f,u,time])\n",
    "            MOR_zvals.append([o,q,a,b,c,d,e,f,u,time])\n",
    "\n",
    "            #plateIDs as well :-\\\n",
    "            MOR_plateIDs.append(int(m))\n",
    "            MOR_plateIDs.append(int(n))\n",
    "    if time == np.max(times):\n",
    "        #to initialise at max time, we get the partitioner and plateIDs of the first step of rotated points\n",
    "        #this isn\\'t technically correct, but as we are only dealing with 1 Ma worth of points, they should fall\n",
    "        #within the correct ocean basin unless at initialisation time the ridge is being subducted (not in our case)\n",
    "        curr_plate_partitioner = pygplates.PlatePartitioner(curr_resolved_topologies, rotation_model)\n",
    "        curr_plateIDs = MOR_plateIDs\n",
    "        curr_lat, curr_lon = init_curr_lat, init_curr_lon\n",
    "        next_lat = (MOR_lat)\n",
    "        next_lon = (MOR_lon)\n",
    "        next_zvals = (MOR_zvals)\n",
    "\n",
    "    if time != np.max(times):\n",
    "        print 'whats been added', len(MOR_lat)\n",
    "        next_lat.extend(MOR_lat)\n",
    "        next_lon.extend(MOR_lon)\n",
    "        next_zvals.extend(MOR_zvals)\n",
    "        \n",
    "            \n",
    "        print len(next_lat)\n",
    "        #print len(next_lat)\n",
    "\n",
    "    print(\"--- %s seconds to timestep ---\" % (tme.time() - start_time))\n",
    "print(\"--- %s seconds overall ---\" % (tme.time() - start_time))\n",
    "\n",
    "#fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2378442.151106552"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_zvals[0][1]*next_zvals[0][2]*1000000/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
