{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the first main step of our methodology. Here we have a pre-existing file (PlateBoundaryTypes) which is a summary of the plate boundaries in the plate model (e.g. lat/lon of start/end points for each segment of a ridge/trench/fault, divergence speed, length, obliquity etc.). We use that data, alongside estimates in the literature of crustal thickness, subdivision of crustal lithologies (e.g. upper/lower volcanics, dykes, gabbros etc.), a parameterisation of serpentinisation etc. to the build the model for carbon storage and serpentinite in ocean crust at ridges, at each time step. In order to properly sample all the parameter space, we do each calcualtion 10000 times per ridge segment. We then save the mean of these values into a big dictionary, which we will access in the next notebook as we start rotating the points through time. We take just the mean because we have the full distribution already saved (3_Uncertainty_Distributions_per_spreading_rate.ipnyb), and we will use that distribution to perturb our result at a trench.\n",
    "\n",
    "In the corresponding paper (Merdith et al. 2019) we wanted to explore the uncertainy inherent in spreading rate for ocean basins that have no recorded spreading history (i.e. prior to the Jurassic mostly). We differentiated our two approahces as (i) plate model spreading rate (PMSR) and (ii) Pacific Ocean spreading rate (POSR).\n",
    "\n",
    "(i) uses the spreading rate in the plate model as it as been constructed \n",
    "(ii) we assume that the spreading rate for the Pacific Ocean since 83 Ma is representative of 'external' oceans (ie. oceans without continental lithosphere and fully ringed by subduction). We built a distribution of expected Pacific Ocean spreading rates (see Notebook 1_Spreading_rate_distributions.ipynb) and instead of using the spreading rate for external oceans in times prior to the Jurassic, we randomly select a spreading rate from that distribution.  As is, these lines are currently blocked out in the below code.\n",
    "\n",
    "References\n",
    "\n",
    "Merdith, A.S., Atkins, S.E., and Tetley, M.G. (2019). Tectonic controls on carbon and serpentinite storage in subducted upper oceanic lithosphere for the past 320 Ma. Frontiers: Earth Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MOR_crust_calculation\n",
    "from scipy import interpolate\n",
    "from collections import defaultdict\n",
    "import MOR_characterisation_serp_flux_FINAL\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select random pacific spreading rate\n",
    "def random_spreading(samples,distribution):\n",
    "    y = np.random.randint(1001,size=samples)\n",
    "    return distribution[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crust_characterisation(start, stop, step, df, POSR=False):\n",
    "\n",
    "    '''\n",
    "    Characterises the crust at each time step using a plate model.\n",
    "    '''\n",
    "    C_storage = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Set up time array\n",
    "    min_time = start\n",
    "    max_time = stop\n",
    "    time_step = step\n",
    "    times = np.arange(min_time, max_time + time_step,time_step)\n",
    "\n",
    "    # deviation angle (in degrees) to split transforms vs. ridges from\n",
    "    deviation_angle= 70\n",
    "    #time resolution to extract data\n",
    "    \n",
    "    time_resolution = 1\n",
    "    SR = 'Spreading Rate'\n",
    "    prop_per = 'Peridotite'\n",
    "    DS = 'Degree of Serpentinisation'\n",
    "    CO2wt_mean = 'CO2 mean'\n",
    "    thick_mean = 'Thickness Mean'\n",
    "    vertical_area_mean = 'Vertical Area Mean'\n",
    "    C_total = 'C-prod'\n",
    "    C_total_vertical = 'C-prod_vertical'\n",
    "    C_volcanic_serp = 'C-Volcanic_serp'\n",
    "    serpentinites_thick = 'Serpentinites_raw'\n",
    "    serpentinites_vertical = 'Serpentinites_raw_vertical'\n",
    "    length = 'Boundary-Length'\n",
    "    boundary = 'Boundary-Type'\n",
    "    StartLat = 'Start-Lat-Point'\n",
    "    StartLon = 'Start-Lon-Point'\n",
    "    EndLat = 'End-Lat-Point'\n",
    "    EndLon = 'End-Lon-Point'\n",
    "    RightPlateID = 'RightPlate'\n",
    "    LeftPlateID = 'LeftPlate'\n",
    "    Index = 'Index'\n",
    "\n",
    "    panthalassa_plate_ids = ['902',\n",
    "                                       '919',\n",
    "                                       '926']\n",
    "\n",
    "    if POSR:\n",
    "        print( 'Using Pacific Ocean Spreading Rates')\n",
    "\n",
    "    else:\n",
    "        print( 'Using Plate Model Spreading Rates')\n",
    "        \n",
    "        #inititate loop to extract data\n",
    "    for time in times:\n",
    "        print( time,  'Ma')\n",
    "        #we only need to cut up spreading ridges by velocity\n",
    "        subset1 = df[(df['Time_Ma']>=time)\n",
    "                  & (df['Time_Ma']<(time+time_resolution))\n",
    "                  & (df['FeatureType']=='gpml:MidOceanRidge')\n",
    "                  & (np.abs(df['Deviation_mod_deg'])<=deviation_angle)]\n",
    "        #print len(subset1)\n",
    "\n",
    "        #create temporary storage for values\n",
    "        C_storage[time][SR] = []\n",
    "        C_storage[time][prop_per] = []\n",
    "        C_storage[time][DS] = []\n",
    "        C_storage[time][CO2wt_mean] = []\n",
    "        C_storage[time][thick_mean] = []\n",
    "        C_storage[time][vertical_area_mean] = []\n",
    "        C_storage[time][C_total] = []\n",
    "        C_storage[time][C_total_vertical] = []\n",
    "        C_storage[time][C_volcanic_serp] = []\n",
    "        C_storage[time][serpentinites_thick] = []\n",
    "        C_storage[time][serpentinites_vertical] = []\n",
    "        C_storage[time][length] = []\n",
    "        C_storage[time][boundary] = []\n",
    "        C_storage[time][StartLat] = []\n",
    "        C_storage[time][StartLon] = []\n",
    "        C_storage[time][EndLat] = []\n",
    "        C_storage[time][EndLon] = []\n",
    "        C_storage[time][RightPlateID] = []\n",
    "        C_storage[time][LeftPlateID] = []\n",
    "        C_storage[time][Index] = []\n",
    "\n",
    "        for index, row in subset1.iterrows():\n",
    "\n",
    "\n",
    "           #returns full spreading rate/velocity in cm/year, times by 10 to convert to km/Ma\n",
    "            velocity = np.ones(samples)\n",
    "            velocity = velocity*row.Plate_Velocity*10\n",
    "\n",
    "            #block out if not using plate model\n",
    "            #for POSR\n",
    "            if POSR:\n",
    "                if time > 160:\n",
    "                    new_conjugate_plates = (row.RightPlate,row.LeftPlate)\n",
    "                    if all(x in panthalassa_plate_ids for x in new_conjugate_plates) == True:\n",
    "                        velocity = np.ones(samples)\n",
    "                        velocity = velocity*random_spreading(1,spread_dist)*10\n",
    "  \n",
    "\n",
    "            #calculate the mean bottom water temperature for the first 20 Ma existence of a parcel of ocean crust\n",
    "            temp_tmp = []\n",
    "            bottom_water_temperature = np.ones(samples)\n",
    "            for i in np.arange(time,time-21,-1):\n",
    "                if i < 0:\n",
    "                    break\n",
    "                temp_tmp.append(bottom_water_temperature_curve[i])\n",
    "            bottom_water_temperature = bottom_water_temperature * np.mean(temp_tmp)\n",
    "\n",
    "            per = MOR_characterisation_serp_flux_FINAL.SR_and_peridotite(samples,\n",
    "                                                                         velocity)\n",
    "\n",
    "            volcanic_percent = 100 - per\n",
    "            asymmetry_factor = 1\n",
    "            calc_length = row.Length_km\n",
    "\n",
    "            #calculate variables for mid ocean ridge segments\n",
    "\n",
    "            calc_length = row.Length_km\n",
    "            width = velocity\n",
    "            #as all velocitie are the same for each point, we just check the first\n",
    "            if velocity[0] <= 40:\n",
    "                asymmetry_factor = 1\n",
    "                thickness = MOR_characterisation_serp_flux_FINAL.SR_and_thickness_slow_ultraslow(samples)\n",
    "\n",
    "                tmp_DS = MOR_characterisation_serp_flux_FINAL.SR_and_dsSurf_slow_ultraslow(samples,\n",
    "                                                                                           velocity,\n",
    "                                                                                          thickness) #DS and thickness is called within this function\n",
    "                #print DS\n",
    "                carbon_max, CO2_gabbro = MOR_characterisation_serp_flux_FINAL.carbon_content_slow_ultraslow(samples,\n",
    "                                                                                                            velocity)\n",
    "                upper_volc_CO2 = lower_volc_CO2 = transition_CO2 = sheeted_dykes_CO2 = 0\n",
    "                upper_volc_thickness = lower_volc_thickness = transition_thickness = sheeted_dykes_thickness = 0\n",
    "                gabbros_thickness = thickness * volcanic_percent/100\n",
    "\n",
    "                #incase a neg CO2 value comes back (i think this was fixed in previous tweaks, but\n",
    "                #leaving it in to be sure)\n",
    "                indices_neg_CO2_gabbro = CO2_gabbro < 0\n",
    "                CO2_gabbro[indices_neg_CO2_gabbro] = 0\n",
    "                CO2_volcanic = gabbros_thickness * CO2_gabbro\n",
    "\n",
    "            else:\n",
    "                carbon_max = 0\n",
    "                tmp_DS = MOR_characterisation_serp_flux_FINAL.SR_and_dsSurf_inter_fast(samples,\n",
    "                                                                                   velocity) #DS and thickness is called within this function\n",
    "\n",
    "                thickness, upper_volc_thickness, lower_volc_thickness, transition_thickness, \\\n",
    "                sheeted_dykes_thickness, gabbros_thickness \\\n",
    "                = MOR_characterisation_serp_flux_FINAL.SR_and_thickness_inter_fast(samples,tmp_DS,volcanic_percent)\n",
    "\n",
    "                upper_volc_CO2, lower_volc_CO2, transition_CO2, sheeted_dykes_CO2, \\\n",
    "                CO2_gabbro, bottom_water_temperature_multiplier = MOR_characterisation_serp_flux_FINAL.carbon_content_inter_fast(samples,\n",
    "                                                                                            velocity,\n",
    "                                                                                            bottom_water_temperature)\n",
    "                #incase a neg CO2 value comes back (i think this was fixed in previous tweaks, but\n",
    "                #leaving it in to be sure)\n",
    "                indices_neg_CO2_gabbro = CO2_gabbro < 0\n",
    "                CO2_gabbro[indices_neg_CO2_gabbro] = 0\n",
    "\n",
    "                #gives us total CO2 storage in volcanics as a fraction (NOT A PERCENT)\n",
    "                CO2_volcanic =  (upper_volc_thickness * upper_volc_CO2 + \\\n",
    "                    lower_volc_thickness * lower_volc_CO2 + \\\n",
    "                    transition_thickness * transition_CO2 + \\\n",
    "                    sheeted_dykes_thickness * sheeted_dykes_CO2 + \\\n",
    "                    gabbros_thickness * CO2_gabbro)/(thickness * volcanic_percent/100)\n",
    "\n",
    "                #incase a neg CO2 value comes back (i think this was fixed in previous tweaks, but\n",
    "                #leaving it in to be sure)\n",
    "                indices_neg_CO2_volcanic = CO2_volcanic < 0\n",
    "                CO2_volcanic[indices_neg_CO2_volcanic] = 0\n",
    "\n",
    "\n",
    "            #carbon in serpentinite\n",
    "            max_DS = 100\n",
    "            #print tmp_DS\n",
    "            if velocity[0] > 40:\n",
    "                 #for fast ridges just assume max C is .32? questionable, no data available i think\n",
    "                carbon_max = .32\n",
    "            m = carbon_max/max_DS\n",
    "            \n",
    "            #bring it all together\n",
    "            C_thickness_volcanics = thickness * volcanic_percent * 1/100 * CO2_volcanic * 1/100 * 12./44.\n",
    "            C_thickness_serpentinites = thickness * per * 1/100 * 1000.0/865.0 * tmp_DS * m *1/100 * 12./44.\n",
    "            C_total_volc_serp =  C_thickness_volcanics + C_thickness_serpentinites\n",
    "\n",
    "            peridotites_point_thickness = thickness * per * 1/100\n",
    "            serpentinites_point_thickness = peridotites_point_thickness * 1000.0/865.0 * tmp_DS *1/100\n",
    "\n",
    "            #print PDF_parameters\n",
    "            C_storage[time][SR].append(np.mean(velocity))\n",
    "            C_storage[time][prop_per].append(np.mean(per))\n",
    "            C_storage[time][DS].append(np.mean(tmp_DS))\n",
    "            C_storage[time][CO2wt_mean].append((np.mean(upper_volc_CO2),\n",
    "                                                 np.mean(lower_volc_CO2),\n",
    "                                                 np.mean(transition_CO2),\n",
    "                                                 np.mean(sheeted_dykes_CO2),\n",
    "                                                 np.mean(CO2_gabbro)))\n",
    "            C_storage[time][thick_mean].append((np.mean(thickness),\n",
    "                                                np.mean(upper_volc_thickness),\n",
    "                                                np.mean(lower_volc_thickness),\n",
    "                                                np.mean(transition_thickness),\n",
    "                                                np.mean(sheeted_dykes_thickness),\n",
    "                                                np.mean(gabbros_thickness),\n",
    "                                                np.mean(peridotites_point_thickness)))\n",
    "            C_storage[time][vertical_area_mean].append((np.mean(upper_volc_thickness * velocity),\n",
    "                                                        np.mean(lower_volc_thickness * velocity),\n",
    "                                                        np.mean(transition_thickness * velocity),\n",
    "                                                        np.mean(sheeted_dykes_thickness * velocity),\n",
    "                                                        np.mean(gabbros_thickness * velocity),\n",
    "                                                        np.mean(peridotites_point_thickness * velocity)))\n",
    "            #multiply by mass C/mass CO2 to get C\n",
    "            C_storage[time][C_total].append(np.mean(C_total_volc_serp))\n",
    "\n",
    "            C_storage[time][C_total_vertical].append(np.mean(C_total_volc_serp)*np.mean(velocity))\n",
    "\n",
    "            C_storage[time][C_volcanic_serp].append((np.mean(C_thickness_volcanics),\n",
    "                                                     np.mean(CO2_volcanic * 12./44.),\n",
    "                                                     np.mean(C_thickness_serpentinites),\n",
    "                                                     np.mean(tmp_DS * m)))\n",
    "\n",
    "            C_storage[time][serpentinites_thick].append(np.mean(serpentinites_point_thickness))\n",
    "\n",
    "            C_storage[time][serpentinites_vertical].append(np.mean(serpentinites_point_thickness) * np.mean(velocity))\n",
    "\n",
    "            C_storage[time][Index].append(index)\n",
    "            C_storage[time][length].append(row.Length_km)\n",
    "            C_storage[time][boundary].append(row.FeatureType)\n",
    "            C_storage[time][StartLat].append(row.StartPointLat)\n",
    "            C_storage[time][StartLon].append(row.StartPointLon)\n",
    "            C_storage[time][EndLat].append(row.EndPointLat)\n",
    "            C_storage[time][EndLon].append(row.EndPointLon)\n",
    "            C_storage[time][RightPlateID].append(row.RightPlate)\n",
    "            C_storage[time][LeftPlateID].append(row.LeftPlate)\n",
    "\n",
    "\n",
    "    #save if wanted\n",
    "    \n",
    "    if POSR:\n",
    "        key='POSR'\n",
    "    else:\n",
    "        key='PMSR'\n",
    "#    date = datetime.today().strftime('%Y-%m-%d')\n",
    "#    filename = 'C_storage_%s_%s.p' % (date,key)\n",
    "#    outfile = open('%s/%s' % (datadir, filename), 'wb')\n",
    "#    pickle.dump(C_storage, outfile)\n",
    "#    outfile.close()\n",
    "\n",
    "    return(C_storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Sample_Data/Distribution_for_random_spreading_Pacific100My.p.py'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '%sDistribution_for_random_spreading_Pacific100My.p.py' % datadir\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file, 'rb') as f:\n",
    "    d = pickle.load(f, encoding='latin1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = './Sample_Data/'\n",
    "savedir = './output/'\n",
    "\n",
    "#NB in cm/a\n",
    "file = '%sDistribution_for_random_spreading_Pacific100My.p.py' % datadir\n",
    "\n",
    "with open(file, 'rb') as f:\n",
    "    spread_dist = pickle.load(f, encoding='latin1') \n",
    "\n",
    "# import previously made file in 'extract velocities'\n",
    "file = '%sPlateBoundaryTypes_410-0_2022Feb21.h5' % datadir\n",
    "df = pd.read_hdf(file,'Statistics_table')\n",
    "tags = df['FeatureType']\n",
    "\n",
    "resolution = 1\n",
    "samples=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bottom water data\n",
    "bottom_water_DF = pd.read_csv('%s/Bottom_water_temp_Muller_et_al.csv' % datadir)\n",
    "#turn into arrays\n",
    "x = np.asarray(bottom_water_DF['Age'])\n",
    "y = np.asarray(bottom_water_DF['Temp'])\n",
    "#flip arrays so they are going back in time\n",
    "x = x[::-1]\n",
    "y = y[::-1]\n",
    "#interpolate into linear 1 Ma intervals\n",
    "f = interpolate.interp1d(x, y)\n",
    "x_new = np.linspace(0,400,401)\n",
    "bottom_water_temperature_curve = f(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Plate Model Spreading Rates\n",
      "0 Ma\n",
      "1 Ma\n",
      "2 Ma\n",
      "3 Ma\n",
      "4 Ma\n",
      "5 Ma\n",
      "6 Ma\n",
      "7 Ma\n",
      "8 Ma\n",
      "9 Ma\n",
      "10 Ma\n",
      "11 Ma\n",
      "12 Ma\n",
      "13 Ma\n",
      "14 Ma\n",
      "15 Ma\n",
      "16 Ma\n",
      "17 Ma\n",
      "18 Ma\n",
      "19 Ma\n",
      "20 Ma\n",
      "21 Ma\n",
      "22 Ma\n",
      "23 Ma\n",
      "24 Ma\n",
      "25 Ma\n",
      "26 Ma\n",
      "27 Ma\n",
      "28 Ma\n",
      "29 Ma\n",
      "30 Ma\n",
      "31 Ma\n",
      "32 Ma\n",
      "33 Ma\n",
      "34 Ma\n",
      "35 Ma\n",
      "36 Ma\n",
      "37 Ma\n",
      "38 Ma\n",
      "39 Ma\n",
      "40 Ma\n",
      "41 Ma\n",
      "42 Ma\n",
      "43 Ma\n",
      "44 Ma\n",
      "45 Ma\n",
      "46 Ma\n",
      "47 Ma\n",
      "48 Ma\n",
      "49 Ma\n",
      "50 Ma\n",
      "51 Ma\n",
      "52 Ma\n",
      "53 Ma\n",
      "54 Ma\n",
      "55 Ma\n",
      "56 Ma\n",
      "57 Ma\n",
      "58 Ma\n",
      "59 Ma\n",
      "60 Ma\n",
      "61 Ma\n",
      "62 Ma\n",
      "63 Ma\n",
      "64 Ma\n",
      "65 Ma\n",
      "66 Ma\n",
      "67 Ma\n",
      "68 Ma\n",
      "69 Ma\n",
      "70 Ma\n",
      "71 Ma\n",
      "72 Ma\n",
      "73 Ma\n",
      "74 Ma\n",
      "75 Ma\n",
      "76 Ma\n",
      "77 Ma\n",
      "78 Ma\n",
      "79 Ma\n",
      "80 Ma\n",
      "81 Ma\n",
      "82 Ma\n",
      "83 Ma\n",
      "84 Ma\n",
      "85 Ma\n",
      "86 Ma\n",
      "87 Ma\n",
      "88 Ma\n",
      "89 Ma\n",
      "90 Ma\n",
      "91 Ma\n",
      "92 Ma\n",
      "93 Ma\n",
      "94 Ma\n",
      "95 Ma\n",
      "96 Ma\n",
      "97 Ma\n",
      "98 Ma\n",
      "99 Ma\n",
      "100 Ma\n",
      "101 Ma\n",
      "102 Ma\n",
      "103 Ma\n",
      "104 Ma\n",
      "105 Ma\n",
      "106 Ma\n",
      "107 Ma\n",
      "108 Ma\n",
      "109 Ma\n",
      "110 Ma\n",
      "111 Ma\n",
      "112 Ma\n",
      "113 Ma\n",
      "114 Ma\n",
      "115 Ma\n",
      "116 Ma\n",
      "117 Ma\n",
      "118 Ma\n",
      "119 Ma\n",
      "120 Ma\n",
      "121 Ma\n",
      "122 Ma\n",
      "123 Ma\n",
      "124 Ma\n",
      "125 Ma\n",
      "126 Ma\n",
      "127 Ma\n",
      "128 Ma\n",
      "129 Ma\n",
      "130 Ma\n",
      "131 Ma\n",
      "132 Ma\n",
      "133 Ma\n",
      "134 Ma\n",
      "135 Ma\n",
      "136 Ma\n",
      "137 Ma\n",
      "138 Ma\n",
      "139 Ma\n",
      "140 Ma\n",
      "141 Ma\n",
      "142 Ma\n",
      "143 Ma\n",
      "144 Ma\n",
      "145 Ma\n",
      "146 Ma\n",
      "147 Ma\n",
      "148 Ma\n",
      "149 Ma\n",
      "150 Ma\n",
      "151 Ma\n",
      "152 Ma\n",
      "153 Ma\n",
      "154 Ma\n",
      "155 Ma\n",
      "156 Ma\n",
      "157 Ma\n",
      "158 Ma\n",
      "159 Ma\n",
      "160 Ma\n",
      "161 Ma\n",
      "162 Ma\n",
      "163 Ma\n",
      "164 Ma\n",
      "165 Ma\n",
      "166 Ma\n",
      "167 Ma\n",
      "168 Ma\n",
      "169 Ma\n",
      "170 Ma\n",
      "171 Ma\n",
      "172 Ma\n",
      "173 Ma\n",
      "174 Ma\n",
      "175 Ma\n",
      "176 Ma\n",
      "177 Ma\n",
      "178 Ma\n",
      "179 Ma\n",
      "180 Ma\n",
      "181 Ma\n",
      "182 Ma\n",
      "183 Ma\n",
      "184 Ma\n",
      "185 Ma\n",
      "186 Ma\n",
      "187 Ma\n",
      "188 Ma\n",
      "189 Ma\n",
      "190 Ma\n",
      "191 Ma\n",
      "192 Ma\n",
      "193 Ma\n",
      "194 Ma\n",
      "195 Ma\n",
      "196 Ma\n",
      "197 Ma\n",
      "198 Ma\n",
      "199 Ma\n",
      "200 Ma\n",
      "201 Ma\n",
      "202 Ma\n",
      "203 Ma\n",
      "204 Ma\n",
      "205 Ma\n",
      "206 Ma\n",
      "207 Ma\n",
      "208 Ma\n",
      "209 Ma\n",
      "210 Ma\n",
      "211 Ma\n",
      "212 Ma\n",
      "213 Ma\n",
      "214 Ma\n",
      "215 Ma\n",
      "216 Ma\n",
      "217 Ma\n",
      "218 Ma\n",
      "219 Ma\n",
      "220 Ma\n",
      "221 Ma\n",
      "222 Ma\n",
      "223 Ma\n",
      "224 Ma\n",
      "225 Ma\n",
      "226 Ma\n",
      "227 Ma\n",
      "228 Ma\n",
      "229 Ma\n",
      "230 Ma\n",
      "231 Ma\n",
      "232 Ma\n",
      "233 Ma\n",
      "234 Ma\n",
      "235 Ma\n",
      "236 Ma\n",
      "237 Ma\n",
      "238 Ma\n",
      "239 Ma\n",
      "240 Ma\n",
      "241 Ma\n",
      "242 Ma\n",
      "243 Ma\n",
      "244 Ma\n",
      "245 Ma\n",
      "246 Ma\n",
      "247 Ma\n",
      "248 Ma\n",
      "249 Ma\n",
      "250 Ma\n",
      "251 Ma\n",
      "252 Ma\n",
      "253 Ma\n",
      "254 Ma\n",
      "255 Ma\n",
      "256 Ma\n",
      "257 Ma\n",
      "258 Ma\n",
      "259 Ma\n",
      "260 Ma\n",
      "261 Ma\n",
      "262 Ma\n",
      "263 Ma\n",
      "264 Ma\n",
      "265 Ma\n",
      "266 Ma\n",
      "267 Ma\n",
      "268 Ma\n",
      "269 Ma\n",
      "270 Ma\n",
      "271 Ma\n",
      "272 Ma\n",
      "273 Ma\n",
      "274 Ma\n",
      "275 Ma\n",
      "276 Ma\n",
      "277 Ma\n",
      "278 Ma\n",
      "279 Ma\n",
      "280 Ma\n",
      "281 Ma\n",
      "282 Ma\n",
      "283 Ma\n",
      "284 Ma\n",
      "285 Ma\n",
      "286 Ma\n",
      "287 Ma\n",
      "288 Ma\n",
      "289 Ma\n",
      "290 Ma\n",
      "291 Ma\n",
      "292 Ma\n",
      "293 Ma\n",
      "294 Ma\n",
      "295 Ma\n",
      "296 Ma\n",
      "297 Ma\n",
      "298 Ma\n",
      "299 Ma\n",
      "300 Ma\n",
      "301 Ma\n",
      "302 Ma\n",
      "303 Ma\n",
      "304 Ma\n",
      "305 Ma\n",
      "306 Ma\n",
      "307 Ma\n",
      "308 Ma\n",
      "309 Ma\n",
      "310 Ma\n",
      "311 Ma\n",
      "312 Ma\n",
      "313 Ma\n",
      "314 Ma\n",
      "315 Ma\n",
      "316 Ma\n",
      "317 Ma\n",
      "318 Ma\n",
      "319 Ma\n",
      "320 Ma\n",
      "321 Ma\n",
      "322 Ma\n",
      "323 Ma\n",
      "324 Ma\n",
      "325 Ma\n",
      "326 Ma\n",
      "327 Ma\n",
      "328 Ma\n",
      "329 Ma\n",
      "330 Ma\n",
      "331 Ma\n",
      "332 Ma\n",
      "333 Ma\n",
      "334 Ma\n",
      "335 Ma\n",
      "336 Ma\n",
      "337 Ma\n",
      "338 Ma\n",
      "339 Ma\n",
      "340 Ma\n",
      "341 Ma\n",
      "342 Ma\n",
      "343 Ma\n",
      "344 Ma\n",
      "345 Ma\n",
      "346 Ma\n",
      "347 Ma\n",
      "348 Ma\n",
      "349 Ma\n",
      "350 Ma\n",
      "351 Ma\n",
      "352 Ma\n",
      "353 Ma\n",
      "354 Ma\n",
      "355 Ma\n",
      "356 Ma\n",
      "357 Ma\n",
      "358 Ma\n",
      "359 Ma\n",
      "360 Ma\n",
      "361 Ma\n",
      "362 Ma\n",
      "363 Ma\n",
      "364 Ma\n",
      "365 Ma\n",
      "366 Ma\n",
      "367 Ma\n",
      "368 Ma\n",
      "369 Ma\n",
      "370 Ma\n",
      "371 Ma\n",
      "372 Ma\n",
      "373 Ma\n",
      "374 Ma\n",
      "375 Ma\n",
      "376 Ma\n",
      "377 Ma\n",
      "378 Ma\n",
      "379 Ma\n",
      "380 Ma\n",
      "381 Ma\n",
      "382 Ma\n",
      "383 Ma\n",
      "384 Ma\n",
      "385 Ma\n",
      "386 Ma\n",
      "387 Ma\n",
      "388 Ma\n",
      "389 Ma\n",
      "390 Ma\n",
      "391 Ma\n",
      "392 Ma\n",
      "393 Ma\n",
      "394 Ma\n",
      "395 Ma\n",
      "396 Ma\n",
      "397 Ma\n",
      "398 Ma\n",
      "399 Ma\n",
      "400 Ma\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import truncnorm\n",
    "C_storage = crust_characterisation(0, 400, 1, df, POSR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "key = 'PMSR'\n",
    "date = datetime.today().strftime('%Y-%m-%d')\n",
    "filename = 'C_storage_%s_%s.p' % (date, key)\n",
    "outfile = open('%s/%s' % (datadir, filename), 'wb')\n",
    "pickle.dump(C_storage, outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
